{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed99fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import global dependencies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a81fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(clients):\n",
    "    clients_power = []\n",
    "    for i in range(clients):\n",
    "        rand = random.randint(1, 100)\n",
    "        clients_power.append(rand)\n",
    "    return clients_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286a0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clients_indexing(clients, clients_power):\n",
    "    # p_11 --> state_1[1]\n",
    "    # p_10 --> state_1[2]\n",
    "    # p_01 --> state_0[1]\n",
    "    # p_00 --> state_0[2]\n",
    "    user_indices = []\n",
    "    for i in range(clients):\n",
    "        if args.clients_state[i] == 1:\n",
    "            v_i_t = -(args.state_1[1]/args.num_users) - (((args.state_1[2]*clients_power[i])/100))\n",
    "            user_indices.append(v_i_t)\n",
    "            # print(f'client {clients[i]}, is in state {clients_state[i]}')\n",
    "        elif args.clients_state[i] == 0:\n",
    "            v_i_t = -(args.state_0[1]/args.num_users) - (((args.state_0[2]*clients_power[i])/100))\n",
    "            user_indices.append(v_i_t)\n",
    "      # print('Indices are', user_indices)\n",
    "      # this prints the top k values\n",
    "        top_k_users = heapq.nlargest(args.top_k, user_indices)\n",
    "      # print(f'the top {top_k} users who can transmit are: {top_k_users}')\n",
    "      # this prints the top k indices\n",
    "        user_indices = np.argsort(user_indices)\n",
    "        top_k_users = user_indices[-args.top_k:]\n",
    "      # print(f'the top {top_k} users who can transmit are: {top_k_users}')\n",
    "      # print(f'client {clients[i]}, is in state {clients_state[i]}')\n",
    "    return top_k_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bca5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wireless_channel_transition_probability(clients):\n",
    "    if args.clients_state == []:\n",
    "        for i in range(clients):\n",
    "            rand_transision = random.random()\n",
    "            if rand_transision <= args.state_0[0]:\n",
    "                args.clients_state.append(0)\n",
    "            else:\n",
    "                args.clients_state.append(1)\n",
    "    else:\n",
    "        for i in range(clients):\n",
    "            rand_transision = random.random()\n",
    "            if args.clients_state[i] == 0:\n",
    "                if rand_transision <= args.state_0[1]:\n",
    "                    args.clients_state[i] = 1\n",
    "                else:\n",
    "                    args.clients_state[i] = 0\n",
    "            else:\n",
    "                if rand_transision <= args.state_0[2]:\n",
    "                    args.clients_state[i] = 0\n",
    "                else:\n",
    "                    args.clients_state[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e9945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class LocalUpdate(object):\n",
    "    def __init__(self, args, dataset=None, idxs=None):\n",
    "        self.args = args\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        # train and update\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n",
    "\n",
    "        epoch_loss = []\n",
    "        for iter in range(self.args.local_ep):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.args.device), labels.to(self.args.device)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images)\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if self.args.verbose and batch_idx % 10 == 0:\n",
    "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n",
    "                               100. * batch_idx / len(self.ldr_train), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff8e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede7652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def FedAvg(w, clients):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            tens = torch.mul(w[i][k], clients[i])\n",
    "            w_avg[k] += tens\n",
    "        w_avg[k] = torch.div(w_avg[k], sum(clients))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa295b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "499f9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(\n",
    "            all_idxs,\n",
    "            random.randint(1,num_items),\n",
    "            replace=False))\n",
    "#         print(len(dict_users[i]))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20405f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6054fb220f4876ad43a0bcfe01f31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c2c964dd64a6ba07ff0d438da316c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c8d54a6f74731be2fd30936b8374b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bf5009568343c7acbc96a92bcbf9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# parse args\n",
    "class args:\n",
    "    gpu = -1 # <- -1 if no GPU is available\n",
    "    device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    num_channels = 1\n",
    "    num_users = 100\n",
    "    top_k = 40\n",
    "    num_classes = 10\n",
    "    frac = 0.1\n",
    "    lr = 0.1\n",
    "    verbose = 0\n",
    "    bs = 128\n",
    "    epochs = 10\n",
    "    \n",
    "    iid = True        # < -This Value needs to be changed\n",
    "    local_ep = 20     # <- This Value needs to be changed\n",
    "    local_bs = 10     # <- This Value needs to be changed\n",
    "    \n",
    "    \n",
    "    state_0 = [0.9449, 0.0087, 0.9913]\n",
    "    state_1 = [0.0551, 0.8509, 0.1491]\n",
    "    \n",
    "    clients_state = []\n",
    "\n",
    "    loss_train_fedavg = []\n",
    "    loss_train_ibcs = []\n",
    "    # training\n",
    "    fedavg_accu = []\n",
    "    fedavg_loss = []\n",
    "    fedavg_power = 0\n",
    "    ibcs_accu = []\n",
    "    ibcs_loss = []\n",
    "    ibcs_power = 0\n",
    "    \n",
    "\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "# sample users\n",
    "if args.iid:\n",
    "    dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "else:\n",
    "    dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "\n",
    "net_glob_fedavg = CNNMnist(args=args).to(args.device)\n",
    "net_glob_ibcs = CNNMnist(args=args).to(args.device)\n",
    "# print(net_glob)\n",
    "net_glob_fedavg.train()\n",
    "net_glob_ibcs.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob_fedavg = net_glob_fedavg.state_dict()\n",
    "w_glob_ibcs = net_glob_ibcs.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4747e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learningFedAvg(epoch):\n",
    "    temp_power = 0\n",
    "    wireless_channel_transition_probability(args.num_users)\n",
    "    \n",
    "    w_locals, loss_locals, num_items = [], [], []\n",
    "    idxs_users = np.random.choice(range(args.num_users), args.top_k, replace=False)\n",
    "#     print(f'selected users {len(idxs_users)}')\n",
    "    for idx in idxs_users:\n",
    "        if args.clients_state[idx] == 0:\n",
    "            num_items.append(len(dict_users[idx]))\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob_fedavg).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "        else:\n",
    "            temp_power += clients_power[idx]\n",
    "    # update global weights\n",
    "    args.w_glob_fedavg = FedAvg(w_locals, num_items)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_fedavg.load_state_dict(w_glob_fedavg)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print(f'Loss avg {loss_avg:.3f}')\n",
    "    \n",
    "    args.loss_train_fedavg.append(loss_avg)\n",
    "    print(f'Train loss {args.loss_train_fedavg}')\n",
    "\n",
    "    # Evaluate score\n",
    "    net_glob_fedavg.eval()\n",
    "    acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(epoch, loss_avg, acc_test))\n",
    "    args.fedavg_accu.append(acc_test)\n",
    "    args.fedavg_loss.append(loss_test)\n",
    "    args.fedavg_power += temp_power\n",
    "    \n",
    "\n",
    "# testing\n",
    "net_glob_fedavg.eval()\n",
    "acc_train, loss_train = test_img(net_glob_fedavg, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8127d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learningIBCS(epoch):\n",
    "    temp_power = 0\n",
    "    wireless_channel_transition_probability(args.num_users)\n",
    "    \n",
    "    w_locals, loss_locals, num_items = [], [], []\n",
    "    idxs_users = clients_indexing(args.num_users, clients_power)\n",
    "#     print(f'selected users {len(idxs_users)}')\n",
    "    for idx in idxs_users:\n",
    "        if args.clients_state[idx] == 0:\n",
    "#             print(f'This is user {idx}')\n",
    "            num_items.append(len(dict_users[idx]))\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob_ibcs).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "        else:\n",
    "            temp_power += clients_power[idx]\n",
    "    # update global weights\n",
    "    args.w_glob_ibcs = FedAvg(w_locals, num_items)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_ibcs.load_state_dict(w_glob_ibcs)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print(f'Loss avg {loss_avg:.3f}')\n",
    "    \n",
    "    args.loss_train_ibcs.append(loss_avg)\n",
    "    print(f'Train loss {args.loss_train_ibcs}')\n",
    "\n",
    "    # Evaluate score\n",
    "    net_glob_ibcs.eval()\n",
    "    acc_test, loss_test = test_img(net_glob_ibcs, dataset_test, args)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(epoch, loss_avg, acc_test))\n",
    "    args.ibcs_accu.append(acc_test)\n",
    "    args.ibcs_loss.append(loss_test)\n",
    "    args.ibcs_power += temp_power\n",
    "    \n",
    "\n",
    "# testing\n",
    "net_glob_ibcs.eval()\n",
    "acc_train, loss_train = test_img(net_glob_ibcs, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob_ibcs, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c545844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Epoch# 1\n",
      "Round   1, Average loss 1.825, Accuracy 11.350\n",
      "This is Epoch# 2\n",
      "Round   2, Average loss 1.372, Accuracy 68.280\n",
      "This is Epoch# 3\n",
      "Round   3, Average loss 0.955, Accuracy 52.760\n",
      "This is Epoch# 4\n",
      "Round   4, Average loss 0.951, Accuracy 68.910\n",
      "This is Epoch# 5\n",
      "Round   5, Average loss 0.997, Accuracy 29.290\n",
      "This is Epoch# 6\n",
      "Round   6, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 7\n",
      "Round   7, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 8\n",
      "Round   8, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 9\n",
      "Round   9, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 10\n",
      "Round  10, Average loss nan, Accuracy 9.800\n",
      "Training accuracy: 9.87\n",
      "Testing accuracy: 9.80\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1, args.epochs+1):\n",
    "    print(f'This is Epoch# {iter}')\n",
    "\n",
    "    w_locals, loss_locals, num_items = [], [], []\n",
    "    idxs_users = np.random.choice(range(args.num_users), args.top_k, replace=False)\n",
    "    for idx in idxs_users:\n",
    "        num_items.append(len(dict_users[idx]))\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob_fedavg).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob_fedavg = FedAvg(w_locals, num_items)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_fedavg.load_state_dict(w_glob_fedavg)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    args.loss_train_fedavg.append(loss_avg)\n",
    "    \n",
    "    # Evaluate score\n",
    "    net_glob_fedavg.eval()\n",
    "    acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
    "\n",
    "# testing\n",
    "net_glob_fedavg.eval()\n",
    "acc_train, loss_train = test_img(net_glob_fedavg, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a713ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Epoch# 1\n",
      "M is: 10\n",
      "Round   1, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 2\n",
      "M is: 10\n",
      "Round   2, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 3\n",
      "M is: 10\n",
      "Round   3, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 4\n",
      "M is: 10\n",
      "Round   4, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 5\n",
      "M is: 10\n",
      "Round   5, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 6\n",
      "M is: 10\n",
      "Round   6, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 7\n",
      "M is: 10\n",
      "Round   7, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 8\n",
      "M is: 10\n",
      "Round   8, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 9\n",
      "M is: 10\n",
      "Round   9, Average loss nan, Accuracy 9.800\n",
      "This is Epoch# 10\n",
      "M is: 10\n",
      "Round  10, Average loss nan, Accuracy 9.800\n",
      "Training accuracy: 9.87\n",
      "Testing accuracy: 9.80\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1, args.epochs+1):\n",
    "    print(f'This is Epoch# {iter}')\n",
    "\n",
    "    w_locals, loss_locals, num_items = [], [], []\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    print(f'M is: {m}')\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in idxs_users:\n",
    "        num_items.append(len(dict_users[idx]))\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob_fedavg).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob_fedavg = FedAvg(w_locals, num_items)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob_fedavg.load_state_dict(w_glob_fedavg)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    args.loss_train_fedavg.append(loss_avg)\n",
    "    \n",
    "    # Evaluate score\n",
    "    net_glob_fedavg.eval()\n",
    "    acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
    "\n",
    "# testing\n",
    "net_glob_fedavg.eval()\n",
    "acc_train, loss_train = test_img(net_glob_fedavg, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob_fedavg, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(args.fedavg_accu)\n",
    "    ax.plot(args.ibcs_accu)\n",
    "\n",
    "    ax.set_title('Accuracy')\n",
    "    ax.legend(['FedAvg', 'IBCS'])\n",
    "    ax.xaxis.set_label_text('Gobal Epochs')\n",
    "    ax.yaxis.set_label_text('Accuracy in %')\n",
    "    plt.show()\n",
    "#     plt.savefig('./results/mnist/Accuracy_MNIST.png')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(args.fedavg_loss)\n",
    "    ax.plot(args.ibcs_loss)\n",
    "\n",
    "    ax.set_title('Loss')\n",
    "    ax.legend(['FedAvg', 'IBCS'])\n",
    "    ax.xaxis.set_label_text('Gobal Epochs')\n",
    "    ax.yaxis.set_label_text('Loss')\n",
    "    # plt.show()\n",
    "    plt.savefig('./results/mnist/Loss_MNIST.png')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(args.fedavg_power)\n",
    "    ax.plot(args.ibcs_power)\n",
    "\n",
    "    ax.set_title('Power')\n",
    "    ax.legend(['FedAvg', 'IBCS'])\n",
    "    ax.xaxis.set_label_text('Gobal Epochs')\n",
    "    ax.yaxis.set_label_text('Power')\n",
    "    # plt.show()\n",
    "    plt.savefig('./results/mnist/Power_MNIST.png')\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850951a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import global dependencies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import random\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9389d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class LocalUpdate(object):\n",
    "    def __init__(self, args, dataset=None, idxs=None):\n",
    "        self.args = args\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        # train and update\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n",
    "\n",
    "        epoch_loss = []\n",
    "        for iter in range(self.args.local_ep):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.args.device), labels.to(self.args.device)\n",
    "                net.zero_grad()\n",
    "                log_probs = net(images)\n",
    "                loss = self.loss_func(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if self.args.verbose and batch_idx % 10 == 0:\n",
    "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n",
    "                               100. * batch_idx / len(self.ldr_train), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5995fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1051224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def FedAvg(w, clients):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            tens = torch.mul(w[i][k], clients[i])\n",
    "            w_avg[k] += tens\n",
    "        w_avg[k] = torch.div(w_avg[k], sum(clients))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faed589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70f86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset) / num_users)\n",
    "    all_idxs = [i for i in range(len(dataset))]  # Initialize all_idxs here\n",
    "    dict_users = {}\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(\n",
    "            all_idxs,\n",
    "            num_items,\n",
    "            replace=False))\n",
    "        print(len(dict_users[i]))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7231f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "600\n",
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# parse args\n",
    "class args:\n",
    "    gpu = -1 # <- -1 if no GPU is available\n",
    "    device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "    num_channels = 1\n",
    "    num_users = 100\n",
    "    num_classes = 10\n",
    "    frac = 0.1\n",
    "    lr = 0.1\n",
    "    verbose = 0\n",
    "    bs = 128\n",
    "    epochs = 100\n",
    "    \n",
    "    iid = True        # < -This Value needs to be changed\n",
    "    local_ep = 20     # <- This Value needs to be changed\n",
    "    local_bs = 10     # <- This Value needs to be changed\n",
    "\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "# sample users\n",
    "if args.iid:\n",
    "    dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "else:\n",
    "    dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "\n",
    "net_glob = CNNMnist(args=args).to(args.device)\n",
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58680b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0, Average loss 1.748, Accuracy 11.400\n",
      "Round   1, Average loss 1.370, Accuracy 56.390\n",
      "Round   2, Average loss 1.285, Accuracy 9.740\n",
      "Round   3, Average loss 1.411, Accuracy 18.060\n",
      "Round   4, Average loss 1.268, Accuracy 37.070\n",
      "Round   5, Average loss 1.260, Accuracy 9.740\n",
      "Round   6, Average loss 2.313, Accuracy 11.350\n",
      "Round   7, Average loss 2.302, Accuracy 10.100\n",
      "Round   8, Average loss 2.304, Accuracy 11.350\n",
      "Round   9, Average loss 2.301, Accuracy 11.350\n",
      "Round  10, Average loss 2.301, Accuracy 11.350\n",
      "Round  11, Average loss 2.301, Accuracy 11.350\n",
      "Round  12, Average loss 2.298, Accuracy 10.280\n",
      "Round  13, Average loss 2.300, Accuracy 10.280\n",
      "Round  14, Average loss 2.301, Accuracy 11.350\n",
      "Round  15, Average loss 2.299, Accuracy 11.350\n",
      "Round  16, Average loss 2.301, Accuracy 11.350\n",
      "Round  17, Average loss 2.301, Accuracy 10.280\n",
      "Round  18, Average loss 2.302, Accuracy 11.350\n",
      "Round  19, Average loss 2.302, Accuracy 11.350\n",
      "Round  20, Average loss 2.303, Accuracy 9.800\n",
      "Round  21, Average loss 2.301, Accuracy 11.350\n",
      "Round  22, Average loss 2.301, Accuracy 11.350\n",
      "Round  23, Average loss 2.300, Accuracy 11.350\n",
      "Round  24, Average loss 2.301, Accuracy 11.350\n",
      "Round  25, Average loss 2.301, Accuracy 11.350\n",
      "Round  26, Average loss 2.302, Accuracy 11.350\n",
      "Round  27, Average loss 2.301, Accuracy 10.280\n",
      "Round  28, Average loss 2.302, Accuracy 11.350\n",
      "Round  29, Average loss 2.303, Accuracy 9.580\n",
      "Round  30, Average loss 2.299, Accuracy 11.350\n",
      "Round  31, Average loss 2.300, Accuracy 11.350\n",
      "Round  32, Average loss 2.303, Accuracy 11.350\n",
      "Round  33, Average loss 2.302, Accuracy 11.350\n",
      "Round  34, Average loss 2.300, Accuracy 11.350\n",
      "Round  35, Average loss 2.300, Accuracy 11.350\n",
      "Round  36, Average loss 2.303, Accuracy 9.820\n",
      "Round  37, Average loss 2.299, Accuracy 11.350\n",
      "Round  38, Average loss 2.302, Accuracy 11.350\n",
      "Round  39, Average loss 2.300, Accuracy 10.280\n",
      "Round  40, Average loss 2.301, Accuracy 10.280\n",
      "Round  41, Average loss 2.301, Accuracy 11.350\n",
      "Round  42, Average loss 2.301, Accuracy 11.350\n",
      "Round  43, Average loss 2.300, Accuracy 10.100\n",
      "Round  44, Average loss 2.301, Accuracy 11.350\n",
      "Round  45, Average loss 2.303, Accuracy 11.350\n",
      "Round  46, Average loss 2.299, Accuracy 11.350\n",
      "Round  47, Average loss 2.302, Accuracy 11.350\n",
      "Round  48, Average loss 2.302, Accuracy 11.350\n",
      "Round  49, Average loss 2.304, Accuracy 11.350\n",
      "Round  50, Average loss 2.303, Accuracy 11.350\n",
      "Round  51, Average loss 2.300, Accuracy 11.350\n",
      "Round  52, Average loss 2.301, Accuracy 11.350\n",
      "Round  53, Average loss 2.301, Accuracy 11.350\n",
      "Round  54, Average loss 2.303, Accuracy 11.350\n",
      "Round  55, Average loss 2.299, Accuracy 11.350\n",
      "Round  56, Average loss 2.300, Accuracy 10.100\n",
      "Round  57, Average loss 2.300, Accuracy 10.090\n",
      "Round  58, Average loss 2.302, Accuracy 11.350\n",
      "Round  59, Average loss 2.303, Accuracy 11.350\n",
      "Round  60, Average loss 2.302, Accuracy 11.350\n",
      "Round  61, Average loss 2.300, Accuracy 11.350\n",
      "Round  62, Average loss 2.304, Accuracy 11.350\n",
      "Round  63, Average loss 2.303, Accuracy 11.350\n",
      "Round  64, Average loss 2.303, Accuracy 11.350\n",
      "Round  65, Average loss 2.301, Accuracy 10.280\n",
      "Round  66, Average loss 2.303, Accuracy 11.350\n",
      "Round  67, Average loss 2.301, Accuracy 11.350\n",
      "Round  68, Average loss 2.300, Accuracy 11.350\n",
      "Round  69, Average loss 2.301, Accuracy 11.350\n",
      "Round  70, Average loss 2.302, Accuracy 11.350\n",
      "Round  71, Average loss 2.301, Accuracy 11.350\n",
      "Round  72, Average loss 2.300, Accuracy 11.350\n",
      "Round  73, Average loss 2.305, Accuracy 11.350\n",
      "Round  74, Average loss 2.301, Accuracy 11.350\n",
      "Round  75, Average loss 2.302, Accuracy 11.350\n",
      "Round  76, Average loss 2.301, Accuracy 11.350\n",
      "Round  77, Average loss 2.301, Accuracy 11.350\n",
      "Round  78, Average loss 2.302, Accuracy 11.350\n",
      "Round  79, Average loss 2.300, Accuracy 11.350\n",
      "Round  80, Average loss 2.301, Accuracy 11.350\n",
      "Round  81, Average loss 2.302, Accuracy 11.350\n",
      "Round  82, Average loss 2.300, Accuracy 11.350\n",
      "Round  83, Average loss 2.302, Accuracy 11.350\n",
      "Round  84, Average loss 2.303, Accuracy 11.350\n",
      "Round  85, Average loss 2.302, Accuracy 10.280\n",
      "Round  86, Average loss 2.301, Accuracy 11.350\n",
      "Round  87, Average loss 2.302, Accuracy 11.350\n",
      "Round  88, Average loss 2.301, Accuracy 11.350\n",
      "Round  89, Average loss 2.300, Accuracy 9.740\n",
      "Round  90, Average loss 2.303, Accuracy 11.350\n",
      "Round  91, Average loss 2.299, Accuracy 11.350\n",
      "Round  92, Average loss 2.305, Accuracy 10.280\n",
      "Round  93, Average loss 2.300, Accuracy 11.350\n",
      "Round  94, Average loss 2.301, Accuracy 11.350\n",
      "Round  95, Average loss 2.301, Accuracy 10.280\n",
      "Round  96, Average loss 2.300, Accuracy 10.280\n",
      "Round  97, Average loss 2.302, Accuracy 11.350\n",
      "Round  98, Average loss 2.301, Accuracy 10.280\n",
      "Round  99, Average loss 2.301, Accuracy 11.350\n",
      "Training accuracy: 11.24\n",
      "Testing accuracy: 11.35\n"
     ]
    }
   ],
   "source": [
    "for iter in range(args.epochs):\n",
    "\n",
    "    w_locals, loss_locals, num_items = [], [], []\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in idxs_users:\n",
    "        num_items.append(len(dict_users[idx]))\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals, idxs_users)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    loss_train.append(loss_avg)\n",
    "    \n",
    "    # Evaluate score\n",
    "    net_glob.eval()\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
    "\n",
    "# testing\n",
    "net_glob.eval()\n",
    "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979ffd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
